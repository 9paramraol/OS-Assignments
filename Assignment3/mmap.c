#include<types.h>#include<mmap.h>#include<page.h>// Paramveer Raol 170459#define PAGE 0x1000/** * Function will invoked whenever there is page fault. (Lazy allocation) *  * For valid acess. Map the physical page  * Return 1 *  * For invalid access, * Return -1.  */u64* os_assignd_addr_pte(struct exec_context *ctx, u64 addr) {	//dump is never used as 0 is passed    u64 *vaddr_base = (u64 *)osmap(ctx->pgd);    u64 *entry;    u32 phy_addr;        entry = vaddr_base + ((addr & PGD_MASK) >> PGD_SHIFT);    phy_addr = (*entry >> PTE_SHIFT) & 0xFFFFFFFF;    vaddr_base = (u64 *)osmap(phy_addr);      /* Address should be mapped as un-priviledged in PGD*/    if( (*entry & 0x1) == 0 || (*entry & 0x4) == 0)        goto out;        entry = vaddr_base + ((addr & PUD_MASK) >> PUD_SHIFT);     phy_addr = (*entry >> PTE_SHIFT) & 0xFFFFFFFF;     vaddr_base = (u64 *)osmap(phy_addr);         /* Address should be mapped as un-priviledged in PUD*/      if( (*entry & 0x1) == 0 || (*entry & 0x4) == 0)          goto out;         entry = vaddr_base + ((addr & PMD_MASK) >> PMD_SHIFT);      phy_addr = (*entry >> PTE_SHIFT) & 0xFFFFFFFF;      vaddr_base = (u64 *)osmap(phy_addr);            /*         Address should be mapped as un-priviledged in PMD          Huge page mapping not allowed      */      if( (*entry & 0x1) == 0 || (*entry & 0x4) == 0 || (*entry & 0x80) == 1)          goto out;           entry = vaddr_base + ((addr & PTE_MASK) >> PTE_SHIFT);            /* Address should be mapped as un-priviledged in PTE*/      if( (*entry & 0x1) == 0 || (*entry & 0x4) == 0)          goto out;                 return entry;out:      return NULL;}int os_assign_unmap_phy_page(struct exec_context *ctx, u64 addr) {    u64 *pte_entry = os_assignd_addr_pte(ctx, addr);    if(!pte_entry)             return -1;      os_pfn_free(USER_REG, (*pte_entry >> PTE_SHIFT) & 0xFFFFFFFF);    *pte_entry = 0;  // Clear the PTE      asm volatile ("invlpg (%0);"                     :: "r"(addr)                     : "memory");   // Flush TLB      return 0;}int change_permisn_pte(struct exec_context * ctx,u64 addr,u32 acc_flags){	u64 *pte_entry = os_assignd_addr_pte(ctx,addr);	if(!pte_entry)		return -1;	*pte_entry = (*pte_entry>>3)<<3 | 0x5;	if( acc_flags & PROT_WRITE )		*pte_entry = *pte_entry | 0x2;	asm volatile ("invlpg (%0);"                     :: "r"(addr)                     : "memory");   // Flush TLB    return 0;	  }void change_permisn_fun(struct exec_context * ctx,u64 s_addr,u64 e_addr,u32 acc_flags){	while( s_addr!= e_addr )	{		u64 *ori_pte =  get_user_pte(ctx, s_addr , 0);     // the origial pte        struct pfn_info * p = get_pfn_info((*ori_pte)>>PAGE_SHIFT);        int refcount = get_pfn_info_refcount(p);                if( refcount==1 )			change_permisn_pte( ctx , s_addr , acc_flags);		asm volatile ("invlpg (%0);"                     :: "r"(s_addr)                     : "memory");   // Flush TLB    		s_addr = s_addr + PAGE;	}	return ;}void dealloc_phy_page(struct exec_context * ctx,u64 s_addr,u64 e_addr){	while( s_addr!=e_addr )	{		u64 *ori_pte =  get_user_pte(ctx, s_addr , 0);     // the origial pte        struct pfn_info * p = get_pfn_info((*ori_pte)>>PAGE_SHIFT);        int refcount = get_pfn_info_refcount(p);                if( refcount==1 )			os_assign_unmap_phy_page(ctx,s_addr);		else		{			*ori_pte = 0;			decrement_pfn_info_refcount(p);		}		asm volatile ("invlpg (%0);"                     :: "r"(s_addr)                     : "memory");   // Flush TLB				s_addr = s_addr + PAGE;	}	return;}void os_asign_page_alloc(struct exec_context *ctx, u64 addr, u32 access_flags) {    u64 *vaddr_base = (u64 *)osmap(ctx->pgd);    u64 *entry;    u64 pfn;    u64 ac_flags = 0x7 ;    entry = vaddr_base + ((addr & PGD_MASK) >> PGD_SHIFT);    if(*entry & 0x1) {      // PGD->PUD Present, access it       pfn = (*entry >> PTE_SHIFT) & 0xFFFFFFFF;       vaddr_base = (u64 *)osmap(pfn);    }else{      // allocate PUD      pfn = os_pfn_alloc(OS_PT_REG);      *entry = (pfn << PTE_SHIFT) | ac_flags;      vaddr_base = osmap(pfn);    }      entry = vaddr_base + ((addr & PUD_MASK) >> PUD_SHIFT);    if(*entry & 0x1) {       // PUD->PMD Present, access it       pfn = (*entry >> PTE_SHIFT) & 0xFFFFFFFF;       vaddr_base = (u64 *)osmap(pfn);    }else{       // allocate PMD       pfn = os_pfn_alloc(OS_PT_REG);       *entry = (pfn << PTE_SHIFT) | ac_flags;       vaddr_base = osmap(pfn);    }     	entry = vaddr_base + ((addr & PMD_MASK) >> PMD_SHIFT);    if(*entry & 0x1) {       // PMD->PTE Present, access it       pfn = (*entry >> PTE_SHIFT) & 0xFFFFFFFF;       vaddr_base = (u64 *)osmap(pfn);    }else{       // allocate PMD       pfn = os_pfn_alloc(OS_PT_REG);       *entry = (pfn << PTE_SHIFT) | ac_flags;       vaddr_base = osmap(pfn);    }      entry = vaddr_base + ((addr & PTE_MASK) >> PTE_SHIFT);   // since this fault occured as frame was not present, we don't need present check here   pfn = os_pfn_alloc(USER_REG);   *entry = (pfn << PTE_SHIFT) | 0x5;   if( access_flags & PROT_WRITE )    	*entry = *entry | 0x2;    return;}void do_alloc_page(struct exec_context * ctx, u64 s_addr, u64 e_addr, u32 access_flags){	while( s_addr != e_addr )	{		os_asign_page_alloc( ctx , s_addr , access_flags );		s_addr = s_addr + PAGE;	}	return ;}int intersection( u64 i1, u64 i2, u64 j1, u64 j2 ){	if( j1==j2 ) // no interval 		return 0;	//j1 and j2 are the intervals basically	//case 1	else if( i1<=j1 && i2>=j2 )		return 1;	//case 2	else if( i1>=j1 && i2<=j2 )		return 1;	//case 3	else if( i1>=j1 && i1<j2 && i2>j2 )		return 1;	//case 4	else if( i2>j1 && i2<=j2 && i1<j1 )		return 1;	// case of no intersection	return 0;}int vm_area_pagefault(struct exec_context *current, u64 addr, int error_code) // does allocation one page at a time{    int fault_fixed = -1;    int permisn=0;    struct vm_area * curr = current->vm_area;    if( addr%PAGE!=0 )    	return -1;    if( error_code==7 )    	return -1;    while( curr )   	{   		if( addr==curr->vm_start || (addr>curr->vm_start && addr<curr->vm_end) )   		{   			if( error_code==6 && ((curr->access_flags&PROT_WRITE)!=0x0) )   				permisn = 1;   			else if( error_code==4 && ((curr->access_flags&PROT_READ)!=0x0) )   				permisn = 1;   			break;   		}   		curr = curr->vm_next;   	}	    	if( permisn==1 )   	{   		u32 acc_flags=curr->access_flags;   		os_asign_page_alloc(current,addr,acc_flags);   		fault_fixed = 1;   	}    return fault_fixed;}/** * mprotect System call Implementation. */int vm_area_mprotect(struct exec_context *current, u64 addr, int length, int prot)  //check 1{	if(length == 0) //some case		return 0;	if(addr<MMAP_AREA_START || addr>MMAP_AREA_END || addr%PAGE!=0 )		return -1;	if( length%PAGE!=0 )		length = (length/PAGE + 1)*PAGE;	if( addr+length > MMAP_AREA_END ) // permissions of area beyond should not be changed		return -1; 	//initial check that all the area is continious	//idea if there exists some intersection with the interval addr and addr+length then return -1	struct vm_area * curr = current->vm_area;	struct vm_area * next ;	struct vm_area * prev ;	int inter_check = 0;	if( curr==NULL )		return -1;	// first intersection is at the begining possiblility	if( intersection(addr,addr+length,MMAP_AREA_START,curr->vm_start)==1 )		inter_check = 1;	// intersection is in the middle	while( curr->vm_next )	{		if( intersection(addr,addr+length,curr->vm_end,curr->vm_next->vm_start)==1 )			inter_check = 1;		curr = curr->vm_next;	}	//intersection is at the end	if( intersection(addr,addr+length,curr->vm_end,MMAP_AREA_END)==1 )		inter_check = 1;	if( inter_check==1 )		return -1;	int alloc_cnt = 0;	curr = current->vm_area;	while( curr )	{		++alloc_cnt;		curr = curr->vm_next;	}	if( alloc_cnt==128 ){		//check if one full section is present in the block		int done = 0;		curr = current->vm_area;		while( curr )		{			if( addr<=curr->vm_start && curr->vm_end<=addr+length)				done=1;			curr = curr->vm_next;		}		// no full block intersectio		if( done==0 ){						//case one block intersection s and e not common			curr = current->vm_area;			while( curr )			{				if(curr->vm_start<addr && addr+length<curr->vm_end && curr->access_flags!=prot)					return -1;				curr = curr->vm_next;			}			//e common but s is not common			curr = current->vm_area;			next = curr->vm_next;			while( curr )			{				if( curr->vm_start<addr && curr->vm_end==addr+length && curr->access_flags!=prot )					if( next==NULL || next->access_flags!=prot )						return -1;				curr = next;				next = next->vm_next;			}			//s is common but e is not			curr = current->vm_area;			prev = NULL;			while( curr )			{				if( curr->vm_start==addr && curr->vm_end>addr+length && curr->access_flags!=prot )					if( prev==NULL || prev->access_flags!=prot )						return -1;				prev = curr;				curr = curr->vm_next;			}			//in the middle situation			curr = current->vm_area;			next = curr->vm_next;			while( next )			{				if( curr->vm_end==next->vm_start && curr->vm_end>addr && curr->vm_end<addr+length)					if( curr->access_flags!=prot && next->access_flags!=prot )						return -1;				curr = next;				next = next->vm_next;			}			//no more situations		}	}		//just permission setting is done	curr=current->vm_area;	while( curr )	{		//case 1		if( curr->vm_start>=addr && curr->vm_end<=addr+length )			curr->access_flags = prot;		//case 2		else if( curr->vm_start<addr && curr->vm_end>addr+length )		{			if( curr->access_flags!=prot)			{				struct vm_area * f_block = alloc_vm_area();				struct vm_area * s_block = alloc_vm_area();				f_block->vm_start = addr;				f_block->vm_end = addr + length;				f_block->access_flags = prot;				f_block->vm_next = s_block;				s_block->vm_start = addr + length;				s_block->vm_end = curr->vm_end;				s_block->access_flags = curr->access_flags;				s_block->vm_next = curr->vm_next;				curr->vm_end = addr;				curr->vm_next = f_block;			}		}		else if( curr->vm_start==addr && curr->vm_end>addr+length )		{			if( curr->access_flags!=prot )			{				struct vm_area * f_block = alloc_vm_area();				f_block->vm_start = addr+length;				f_block->vm_end = curr->vm_end;				f_block->access_flags = curr->access_flags;				f_block->vm_next = curr->vm_next;				curr->vm_end = addr + length;				curr->access_flags = prot;				curr->vm_next = f_block;			}		}		else if( curr->vm_start<addr && curr->vm_end==addr+length)		{			if( curr->access_flags!=prot)			{				struct vm_area * f_block = alloc_vm_area();				f_block->vm_start = addr;				f_block->vm_end = addr + length;				f_block->access_flags = prot;				f_block->vm_next = curr->vm_next;				curr->vm_end = addr;				curr->vm_next = f_block;			}		}		//case 3		else if( curr->vm_start>=addr && curr->vm_start<addr+length && curr->vm_end>addr+length)		{			if( curr->access_flags!=prot)			{				struct vm_area * f_block = alloc_vm_area();				f_block->vm_start = addr + length;				f_block->vm_end = curr->vm_end;				f_block->access_flags = curr->access_flags;				f_block->vm_next = curr->vm_next;				curr->vm_end = addr + length;				curr->access_flags = prot;				curr->vm_next = f_block;			}		}		//case 4 		else if( curr->vm_end>addr && curr->vm_end<=addr+length && curr->vm_start<addr )		{			if( curr->access_flags!=prot )			{				struct vm_area * f_block = alloc_vm_area();				f_block->vm_start = addr;				f_block->vm_end = curr->vm_end;				f_block->access_flags = prot;				f_block->vm_next = curr->vm_next;				curr->vm_end = addr;				curr->vm_next = f_block;			}		}		curr = curr->vm_next;	}	//final coalesing	curr = current->vm_area;	while ( curr )	{		next = curr->vm_next;				if( next!=NULL && curr->vm_end==next->vm_start && curr->access_flags==next->access_flags )		{				curr->vm_end = next->vm_end;			curr->vm_next = next->vm_next;			dealloc_vm_area(next);			continue;		}		curr=curr->vm_next;	}	u32 acc_flags = prot;	u64 end = addr + length;	change_permisn_fun( current , addr , end , acc_flags);  	return 0;}/** * mmap system call implementation. */long vm_area_map(struct exec_context *current, u64 addr, int length, int prot, int flags){	//making page aligned	if( length%PAGE != 0 )		length = (length/PAGE + 1)*PAGE; 		int alloc_cnt = 0;	struct vm_area * alloc_curr = current->vm_area;	while( alloc_curr )	{		++ alloc_cnt;		alloc_curr = alloc_curr->vm_next;	}	//case one when addr is null	if  ( addr == 0x0 )   // no hard and fast rule to allocate	{		//case when nothing has been allocated till now		if( current->vm_area == NULL ){			if( MMAP_AREA_START+length > MMAP_AREA_END )				return -1;						struct vm_area* f_block = alloc_vm_area();			f_block->vm_start  =  MMAP_AREA_START;			f_block->vm_end = MMAP_AREA_START + length ;			f_block->access_flags = prot;			f_block->vm_next = NULL;			current->vm_area = f_block;			if( (flags & MAP_POPULATE) != 0 )				do_alloc_page(current,f_block->vm_start,f_block->vm_end,f_block->access_flags);			return MMAP_AREA_START;		}		//case when vm area is not null		else		{			//when memory is avilable at the starting 			if( current->vm_area->vm_start >= (MMAP_AREA_START + length) )			{				long int end = MMAP_AREA_START + length ;				//if merger possible then merge				if( end == current->vm_area->vm_start && prot==(current->vm_area->access_flags))				{					current->vm_area->vm_start = MMAP_AREA_START;					if( flags & MAP_POPULATE )						do_alloc_page(current,MMAP_AREA_START,end,current->vm_area->access_flags);					return MMAP_AREA_START;				}				//case when merger is not possible				if( alloc_cnt==128 )					return -1;				struct vm_area * f_block = alloc_vm_area();				f_block->vm_start = MMAP_AREA_START;				f_block->vm_end = end;				f_block->access_flags = prot;				f_block->vm_next = current->vm_area;				current->vm_area = f_block;				if( flags & MAP_POPULATE )					do_alloc_page(current,MMAP_AREA_START,end,f_block->access_flags);				return MMAP_AREA_START; 			}			//when memory is avilabe in the middle			struct vm_area * curr = current->vm_area;			struct vm_area * next = current->vm_area->vm_next;			while( next )			{				//3 merger possible case				if( (next->vm_start - curr->vm_end ) == length )				{					u64 start=curr->vm_end;					u64 end=next->vm_start;					u32 acc_flags = prot;					if(prot==curr->access_flags && prot==next->access_flags)					{							curr->vm_end = next->vm_end;						curr->vm_next = next->vm_next;						dealloc_vm_area(next); 					}					else if( prot==curr->access_flags )						curr->vm_end = next->vm_start ;					else if( prot==next->access_flags )						next->vm_start = curr->vm_end ;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block = alloc_vm_area();						f_block->vm_start = curr->vm_end ;						f_block->vm_end = next->vm_start ;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;					}					if( MAP_POPULATE & flags )						do_alloc_page(current,start,end,acc_flags);					return  (long)start; 				}				//2 merger only possible case				else if( (next->vm_start - curr->vm_end ) > length )				{					u64 start=curr->vm_end;					u64 end=curr->vm_end + length;					u32 acc_flags = prot;						if( prot==curr->access_flags )						curr->vm_end = curr->vm_end + length;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block = alloc_vm_area();						f_block->vm_start = curr->vm_end ;						f_block->vm_end = curr->vm_end + length;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;					}					if( MAP_POPULATE & flags )						do_alloc_page(current,start,end,acc_flags);					return (long)start;				}				//always else problem will occur an infinite loop moving forward									curr = next;					next = next->vm_next;							}			//case when addition can be done only at end			if( curr->vm_end+length > MMAP_AREA_END)				return -1;			else			{				//merging possible				u64 start=curr->vm_end;				u64 end=curr->vm_end+length;				u32 acc_flags = prot;				if( prot == curr->access_flags)						curr->vm_end = curr->vm_end + length;				else				{					if( alloc_cnt==128 )						return -1;					struct vm_area * f_block = alloc_vm_area();					f_block->vm_start = curr->vm_end ;					f_block->vm_end = curr->vm_end + length;					f_block->access_flags = prot;					f_block->vm_next = NULL;					curr->vm_next = f_block;				}				if( MAP_POPULATE & flags )					do_alloc_page(current,start,end,acc_flags);				return (long)start; 			}		}	}	//when address is not null	else if(addr!=0)	{		//when addr given is not right		if( addr+length<=MMAP_AREA_START || addr+length>MMAP_AREA_END )			return -1;		if( addr<MMAP_AREA_START || addr>=MMAP_AREA_END )			return -1;		//ADDRESS IS FREE 		if( current->vm_area == NULL )		{			struct vm_area * f_block = alloc_vm_area();			f_block->vm_start = addr;			f_block->vm_end = addr + length;			f_block->access_flags = prot;			f_block->vm_next = NULL;			current->vm_area = f_block;			u32 acc_flags = prot;						if( flags & MAP_POPULATE )				do_alloc_page(current,addr,addr+length,acc_flags);			return addr;		}		//address is free and at the begining		if( addr+length <= current->vm_area->vm_start )		{			//merging possible			if( addr+length==current->vm_area->vm_start && prot==current->vm_area->access_flags )				current->vm_area->vm_start = addr;			else			{				if( alloc_cnt==128 )					return -1;				struct vm_area * f_block = alloc_vm_area();				f_block->vm_start = addr;				f_block->vm_end = addr + length;				f_block->access_flags = prot;				f_block->vm_next = current->vm_area;				current->vm_area = f_block;			}			u32 acc_flags = prot;			if( flags & MAP_POPULATE )				do_alloc_page(current,addr,addr+length,acc_flags);			return addr;		}		//address is free but in middle		struct vm_area * curr = current->vm_area;		struct vm_area * next = current->vm_area->vm_next;		while( next )		{			//3 merger possible case			if( (next->vm_start)==(addr+length) &&  (curr->vm_end)==addr ) 			{				if(prot==curr->access_flags && prot==next->access_flags)				{					curr->vm_end = next->vm_end;					curr->vm_next = next->vm_next;					dealloc_vm_area(next); 				}				else if( prot==curr->access_flags )					curr->vm_end = next->vm_start ;				else if( prot==next->access_flags )					next->vm_start = curr->vm_end ;				else				{					if( alloc_cnt==128 )						return -1;					struct vm_area * f_block = alloc_vm_area();					f_block->vm_start = curr->vm_end ;					f_block->vm_end = next->vm_start ;					f_block->access_flags = prot;					f_block->vm_next = next;					curr->vm_next = f_block;				}				u32 acc_flags = prot;				if( MAP_POPULATE  & flags)					do_alloc_page(current,addr,addr+length,acc_flags);				return  addr; 			}			//2 merger only possible case			else if( (next->vm_start)>=addr+length && (curr->vm_end)<=addr )			{				//first's end common				if( (curr->vm_end)==addr )				{					if( prot==curr->access_flags )						curr->vm_end = addr + length;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block = alloc_vm_area();						f_block->vm_start = addr;						f_block->vm_end = addr + length;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;					}				}				//next's begining common				else if( (next->vm_start)==addr+length )				{					if( prot==next->access_flags )						next->vm_start = addr;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block = alloc_vm_area();						f_block->vm_start = addr;						f_block->vm_end = addr + length;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;					} 				}				//nothing in common				else				{					if( alloc_cnt==128 )						return -1;					struct vm_area * f_block = alloc_vm_area();					f_block->vm_start = addr;					f_block->vm_end = addr + length;					f_block->access_flags = prot;					f_block->vm_next = next;					curr->vm_next = f_block;				}				return addr;			}			//moving forward			else			{				curr = next;				next = next->vm_next;			}		}		//address is free but at end		if( curr->vm_end<=addr && (addr+length)<=MMAP_AREA_END )		{			//curr's end is same as the addr			if( curr->vm_end == addr && curr->access_flags==prot )				curr->vm_end = addr + length;			else			{				if( alloc_cnt==128 )					return -1;				struct vm_area * f_block = alloc_vm_area();					f_block->vm_start = addr;				f_block->vm_end = addr + length;				f_block->access_flags = prot;				f_block->vm_next = NULL;				curr->vm_next = f_block;			}			u32 acc_flags = prot;			if( MAP_POPULATE & flags )				do_alloc_page(current,addr,addr+length,acc_flags);			return addr;					}		//exact address not avilable		if( flags & MAP_FIXED != 0 )			return -1;				//ADDRESS IS NOT FREE since even after above steps we don't get any address that is free				//address asked is at the begining and the space is not there		if( addr<current->vm_area->vm_start  && addr+length>current->vm_area->vm_start)		{			//same algo as when in not given case			curr = current->vm_area;			next = curr->vm_next;			while( next )			{				//3 merging possibility				if( (next->vm_start - curr->vm_end)==length )				{					u64 start = curr->vm_end;					u64 end = next->vm_start;					u32 acc_flags = prot;					if( prot==curr->access_flags && prot==next->access_flags )					{						curr->vm_end = next->vm_end;						curr->vm_next = next->vm_next;						dealloc_vm_area(next); 					}					else if(curr->access_flags==prot)						curr->vm_end = next->vm_start;					else if(next->access_flags==prot)						next->vm_start=curr->vm_end;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block=alloc_vm_area();						f_block->vm_start = curr->vm_end;						f_block->vm_end = next->vm_start;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;					}					if( MAP_POPULATE & flags )						do_alloc_page(current,start,end,acc_flags);					return (long)start;				}				//2 merging possiblity				else if( (next->vm_start - curr->vm_end)>length )				{					u64 start = curr->vm_end;					u64 end = start + length;					u32 acc_flags = prot; //b remember					if( curr->access_flags==prot )						curr->vm_end = curr->vm_end + length;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block=alloc_vm_area();						f_block->vm_start = curr->vm_end;						f_block->vm_end = curr->vm_end + length;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;					}					if( MAP_POPULATE & flags )						do_alloc_page(current,start,end,acc_flags);					return (long)start;				}				// nothing 				else				{					curr = next;					next = next->vm_next;				}			}			// finally allocation at end			if( curr->vm_end + length <= MMAP_AREA_END )			{				u64 start = curr->vm_end;				u64 end = start + length;				u32 acc_flags = prot; 				if( curr->access_flags==prot )					curr->vm_end = curr->vm_end + length;				else				{					if( alloc_cnt==128 )						return -1;					struct vm_area * f_block=alloc_vm_area();					f_block->vm_start = curr->vm_end;					f_block->vm_end = curr->vm_end + length;					f_block->access_flags = prot;					f_block->vm_next = NULL;					curr->vm_next = f_block;				}				if( MAP_POPULATE & flags )					do_alloc_page(current,start,end,acc_flags);				return (long)start;			}			return -1;		}		//address asked is in middle and space is not avilable		curr = current->vm_area;		next = curr->vm_next;		int alloc_flag = 0;		while( next )		{			if( curr->vm_start<=addr && next->vm_start>addr ){				alloc_flag = 1;				break;			}			else			{				curr = next;				next = next->vm_next;			}		}		if( alloc_flag == 1)		{	//free block required in middle			while( next )			{				//3 merging possibility				if( (next->vm_start - curr->vm_end)==length )				{					u64 start = curr->vm_end;					u64 end = start + length;					u32 acc_flags = prot;					if( prot==curr->access_flags && prot==next->access_flags )					{						curr->vm_end = next->vm_end;						curr->vm_next = next->vm_next;						dealloc_vm_area(next); 					}					else if(curr->access_flags==prot)						curr->vm_end = next->vm_start;					else if(next->access_flags==prot)						next->vm_start=curr->vm_end;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block=alloc_vm_area();						f_block->vm_start = curr->vm_end;						f_block->vm_end = next->vm_start;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;					}					if( MAP_POPULATE & flags )						do_alloc_page(current,start,end,acc_flags);					return (long)start;				}				//2 merging possiblity				else if( (next->vm_start - curr->vm_end)>length )				{					u64 start = curr->vm_end;					u64 end = start + length;					u32 acc_flags = prot; 					if( prot==curr->access_flags)						curr->vm_end = curr->vm_end + length;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block=alloc_vm_area();						f_block->vm_start = curr->vm_end;						f_block->vm_end = curr->vm_end + length;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;						}					if( MAP_POPULATE & flags )						do_alloc_page(current,start,end,acc_flags);					return (long)start;				}				else				{					curr = next;					next = next->vm_next;				}							} 			//free block at end			if( curr->vm_end+length <= MMAP_AREA_END)			{				u64 start = curr->vm_end;				u64 end = start + length;				u32 acc_flags = prot;				if( curr->access_flags == prot )					curr->vm_end = curr->vm_end + length;				else				{					if( alloc_cnt==128 )							return -1;					struct vm_area * f_block=alloc_vm_area();					f_block->vm_start = curr->vm_end;					f_block->vm_end = curr->vm_end + length;					f_block->access_flags = prot;					f_block->vm_next = NULL;					curr->vm_next = f_block;									}				if( MAP_POPULATE & flags )					do_alloc_page(current,start,end,acc_flags);				return (long)start;			}			//free block at the begining			curr = current->vm_area;			next = curr->vm_next;			if( MMAP_AREA_START+length <= curr->vm_start )			{				u64 start = MMAP_AREA_START;				u64 end = start + length;				u32 acc_flags = prot;				if( curr->access_flags==prot && MMAP_AREA_START+length==curr->vm_start )					curr->vm_start = MMAP_AREA_START;				else				{					if( alloc_cnt==128 )							return -1;					struct vm_area * f_block=alloc_vm_area();					f_block->vm_start = MMAP_AREA_START;					f_block->vm_end = MMAP_AREA_START + length;					f_block->access_flags = prot;					f_block->vm_next = curr;					current->vm_area = f_block;				}				if( MAP_POPULATE & flags )					do_alloc_page(current,start,end,acc_flags);				return (long)MMAP_AREA_START;			}			//free block is avilable in middle			while ( next )			{							if( curr->vm_start > addr)					break;				if( (next->vm_start - curr->vm_end)==length )				{					u64 start = curr->vm_end;					u64 end = start + length;					u32 acc_flags = prot;					if( prot==curr->access_flags && prot==next->access_flags )					{						curr->vm_end = next->vm_end;						curr->vm_next = next->vm_next;						dealloc_vm_area(next); 					}					else if(curr->access_flags==prot)						curr->vm_end = next->vm_start;					else if(next->access_flags==prot)						next->vm_start=curr->vm_end;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block=alloc_vm_area();						f_block->vm_start = curr->vm_end;						f_block->vm_end = next->vm_start;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;					}					if( MAP_POPULATE & flags )						do_alloc_page(current,start,end,acc_flags);					return (long)start;				}				//2 merging possiblity				else if( (next->vm_start - curr->vm_end)>length )				{					u64 start = curr->vm_end;					u64 end = start + length;					u32 acc_flags = prot; 					if( prot==curr->access_flags)						curr->vm_end = curr->vm_end + length;					else					{						if( alloc_cnt==128 )							return -1;						struct vm_area * f_block=alloc_vm_area();						f_block->vm_start = curr->vm_end;						f_block->vm_end = curr->vm_end + length;						f_block->access_flags = prot;						f_block->vm_next = next;						curr->vm_next = f_block;						}					if( MAP_POPULATE & flags )						do_alloc_page(current,start,end,acc_flags);					return (long)start;				}				else				{					curr = next;					next = next->vm_next;				}							}			return -1; 		}		//address is at the end of vm _area		//space between last part and the map_end avilable		if( curr->vm_start<=addr && (curr->vm_end+length)<=MMAP_AREA_END )		{						u64 start = curr->vm_end;			u64 end = curr->vm_end+length;			u32 acc_flags = prot;			if( curr->access_flags==prot )				curr->vm_end = curr->vm_end + length;			else			{				if( alloc_cnt==128 )							return -1;				struct vm_area * f_block = alloc_vm_area();				f_block->vm_start = curr->vm_end;				f_block->vm_end = f_block->vm_start + length;				f_block->access_flags = prot;				f_block->vm_next = NULL;				curr->vm_next = f_block;			}			if( MAP_POPULATE & flags )				do_alloc_page(current,start,end,acc_flags);			return (long)start;					}		curr = current->vm_area;		next = curr->vm_next;		if( MMAP_AREA_START+length <= curr->vm_start )		{				u64 start = MMAP_AREA_START;				u64 end = start + length;				u32 acc_flags = prot;				if( curr->access_flags==prot && MMAP_AREA_START+length==curr->vm_start )					curr->vm_start = MMAP_AREA_START;				else				{					if( alloc_cnt==128 )							return -1;					struct vm_area * f_block=alloc_vm_area();					f_block->vm_start = MMAP_AREA_START;					f_block->vm_end = MMAP_AREA_START + length;					f_block->access_flags = prot;					f_block->vm_next = curr;					current->vm_area = f_block;				}				if( MAP_POPULATE & flags )					do_alloc_page(current,start,end,acc_flags);				return (long)MMAP_AREA_START;		}		//free block is avilable in middle		while ( next )		{			//3 merger possible			//3 merging possibility			if( (next->vm_start - curr->vm_end)==length )			{				u64 start = curr->vm_end;				u64 end = start + length;				u32 acc_flags = prot;				if( prot==curr->access_flags && prot==next->access_flags )				{					curr->vm_end = next->vm_end;					curr->vm_next = next->vm_next;					dealloc_vm_area(next); 				}				else if(curr->access_flags==prot)					curr->vm_end = next->vm_start;				else if(next->access_flags==prot)					next->vm_start=curr->vm_end;				else				{					if( alloc_cnt==128 )							return -1;					struct vm_area * f_block=alloc_vm_area();					f_block->vm_start = curr->vm_end;					f_block->vm_end = next->vm_start;					f_block->access_flags = prot;					f_block->vm_next = next;					curr->vm_next = f_block;				}				if( MAP_POPULATE & flags )					do_alloc_page(current,start,end,acc_flags);				return (long)start;			}			//2 merging possiblity			else if( (next->vm_start - curr->vm_end)>length )			{				u64 start = curr->vm_end;				u64 end = start + length;				u32 acc_flags = prot; 				if( prot==curr->access_flags)					curr->vm_end = curr->vm_end + length;				else				{					if( alloc_cnt==128 )							return -1;					struct vm_area * f_block=alloc_vm_area();					f_block->vm_start = curr->vm_end;					f_block->vm_end = curr->vm_end + length;					f_block->access_flags = prot;					f_block->vm_next = next;					curr->vm_next = f_block;					}				if( MAP_POPULATE & flags )					do_alloc_page(current,start,end,acc_flags);				return (long)start;			}			else			{				curr = next;				next = next->vm_next;			}				}		return -1; 			}	return -1;    }/** * munmap system call implemenations */int vm_area_unmap(struct exec_context *current, u64 addr, int length){	if( length%PAGE!=0 )    	length = (length/PAGE + 1)*PAGE;    if( addr<MMAP_AREA_START || addr>=MMAP_AREA_END )    	return -1;    if( addr+length>MMAP_AREA_END ) //should give error as one cannot dealloc other memory    	return -1;	int alloc_cnt = 0;	struct vm_area * alloc_curr = current->vm_area;	while( alloc_curr )	{		++ alloc_cnt;		alloc_curr = alloc_curr->vm_next;	}       // if a page contains part of address space then its vm_start and vm_end is set to -1    struct vm_area * curr = current->vm_area;    //marking stuff    while( curr )    {    	if( curr->vm_start > addr+length )    		break;    	// entire segment within the defined range    	if( curr->vm_start>=addr && curr->vm_end<=addr+length)    	{    		curr->vm_start = -1;    		curr->vm_end = -1;    	}    	//start part only in segment    	else if( curr->vm_start>=addr && curr->vm_start<addr+length && curr->vm_end>addr+length)    		curr->vm_start = addr + length;    	//end part only in the segment    	else if( curr->vm_end>addr && curr->vm_end<=addr+length && curr->vm_start<addr)    		curr->vm_end = addr;    	//fullon overlap the only case just output then and there    	else if( curr->vm_start<addr && curr->vm_end>addr+length )    	{    		if( alloc_cnt==128 )    			return -1;    		struct vm_area * f_block = alloc_vm_area();    		f_block->vm_start = addr + length;    		f_block->vm_end = curr->vm_end;    		curr->vm_end = addr;    		f_block->access_flags = curr->access_flags;    		f_block->vm_next = curr->vm_next;    		curr->vm_next = f_block;    		dealloc_phy_page(current,addr,addr+length);    		return 0;    	}    	curr = curr->vm_next;    }    //deleting stuff if present at starting    // all -1s will be contagious so     if( current->vm_area->vm_start==-1 )    {	    while( current->vm_area )	    {		    	curr = current->vm_area;	    	if( curr->vm_start==-1 && curr->vm_end==-1 ){	    		current->vm_area = curr->vm_next;	    		dealloc_vm_area(curr);	    	}	    	else	    		break; 	    }	    dealloc_phy_page(current,addr,addr+length);	    return 0;	}	else	{		//finding first point from where -1 starts		curr = current->vm_area;		struct vm_area * next = curr->vm_next;		while( next )		{			if( next->vm_start==-1 && next->vm_end==-1 )				break;			curr=next;			next=next->vm_next;		}		while( next )		{			if( next->vm_start==-1 && next->vm_end==-1 )			{				curr->vm_next = next->vm_next;				dealloc_vm_area(next);				next = curr->vm_next;			}			else				break;		}		dealloc_phy_page(current,addr,addr+length);		return 0;	}	return 0;	}	